{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Starting & Ending Trip Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're doing analysis and visualization on trips from 01/01/2018-01/01/2019. There are 156,602 Start or End bike events. By the end of this notebook, we have determined how bike "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156602 Starting/Ending trip bike events.\n",
      "24 unique Latitude Coordinates.\n",
      "24 unique Longitude Coordinates.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from datetime import datetime\n",
    "df = pd.read_csv('../data/mbike_NamedStations_StartEndOnly.csv', index_col=0)\n",
    "\n",
    "print(str(len(df.index)) + \" Starting/Ending trip bike events.\")\n",
    "print(str(len(df.Coords_Latitude.unique())) + \" unique Latitude Coordinates.\")\n",
    "print(str(len(df.Coords_Longitude.unique())) + \" unique Longitude Coordinates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date_time column converted to a python datetime object, then get the day of the week this corresponds to.  \n",
    "#### Then split the datsaset into weekdays & weekdays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f7a9739116cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date_Time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixDateTime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date_Time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date_Time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coerce'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Day'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date_Time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdayofweek\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   6012\u001b[0m                          \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6013\u001b[0m                          kwds=kwds)\n\u001b[0;32m-> 6014\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6016\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;31m# compute the result using the series generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-f7a9739116cc>\u001b[0m in \u001b[0;36mfixDateTime\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdatetime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#print(date)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date_Time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixDateTime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 data = _sanitize_array(data, index, dtype, copy,\n\u001b[0;32m--> 275\u001b[0;31m                                        raise_cast_failure=True)\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m   4120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4121\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4122\u001b[0;31m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_platform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4124\u001b[0m         \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_cast_to_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mmaybe_convert_platform\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_1d_object_array_from_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dtype'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_values'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mconstruct_1d_object_array_from_listlike\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   1225\u001b[0m     \u001b[0;31m# numpy will try to interpret nested lists as further dimensions, hence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \u001b[0;31m# making a 1D array that contains list-likes is a bit tricky:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1228\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def fixDateTime(row):\n",
    "    s = row['Date_Time'].split(\"T\")\n",
    "    t = s[1].split(\".\")[0]\n",
    "    date = s[0]\n",
    "    datetime = date +\" \" + t\n",
    "    #print(date)\n",
    "    return pd.Series([datetime])\n",
    "\n",
    "df[['Date_Time']] = df.apply(fixDateTime, axis=1)\n",
    "df['Date_Time'] = pd.to_datetime(df['Date_Time'], errors='coerce')\n",
    "df['Day'] = pd.to_datetime(df['Date_Time']).dt.dayofweek\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Day\", data= df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*Distribution of usage by day. 0 corresponds to Monday, 6 corresponds to Sunday.*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User_ID counts\n",
    "Looks like most users will only take 25-50 trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(df['User_ID'].value_counts()[:], norm_hist=False, kde=False)\n",
    "plt.xlim(0,400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().Date_Time.dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 7]\n",
    "sns.countplot(df.Bike_Event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Bike_Event.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 15]\n",
    "ax = sns.countplot(y=\"StationName\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStation = pd.read_csv('../data/stationsInfo.csv')\n",
    "dfStation.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.countplot(y=\"StationName\")\n",
    "ax = sns.barplot(y=\"Name\", x=\"Altitude\", data=dfStation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original length of start+end trip events (~156000)\n",
    "df = df[df['Date_Time'] > datetime(2018,1,1)] #After 01/2018 had 23 stations!!!!\n",
    "print(len(df.index)) # 104031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into Start/Ends\n",
    "# keep only if after 03/2018 ******\n",
    "dfStart = df[df[\"Bike_Event\"] == 'StartTrip'] # 56479\n",
    "dfEnd = df[df[\"Bike_Event\"]!= 'StartTrip']  # 57485,\n",
    "# So, -723 (More ends than starts). Weight for ends:\n",
    "print(len(dfStart.index)/len(dfEnd.index))\n",
    "print(len(dfStart.index))\n",
    "print(len(dfEnd.index))\n",
    "\n",
    "weight = len(dfStart.index)/len(dfEnd.index)\n",
    "dfStart.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How correlated is altitude to usage?\n",
    "\n",
    "dfPlot = pd.DataFrame(dfStart.StationName.value_counts())#Returns a series in descending order, converts to df\n",
    "dfPlot1 = pd.DataFrame(dfEnd.StationName.value_counts())\n",
    "dfPlot.columns = ['StartFreq']\n",
    "dfPlot1.columns = ['EndFreq']\n",
    "\n",
    "dfmergeS = dfStation.join(dfPlot, on='Name')\n",
    "# dfmergeSE consists of has staion info + the number of start and end trips.\n",
    "dfmergeSE = dfmergeS.join(dfPlot1, on='Name')\n",
    "dfmergeSE['Diff'] = dfmergeSE['StartFreq']-dfmergeSE['EndFreq']\n",
    "dfmergeSE['Total'] = dfmergeSE['StartFreq']+dfmergeSE['EndFreq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmergeSE.Diff.sum()\n",
    "dfmergeSE.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfmergeSE['Diff_Norm'] = dfmergeSE['StartFreq']-weight*dfmergeSE['EndFreq']\n",
    "dfmergeSE.Diff_Norm.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.lmplot(x='Altitude', y='StartFreq', data=dfmergeSE)\n",
    "from scipy import stats\n",
    "# Use scipy.stats to get the linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(dfmergeSE['Altitude'],dfmergeSE['StartFreq'])\n",
    "\n",
    "# Pass parameters of fit using line_kws for legend\n",
    "ax = sns.regplot(x=\"Altitude\", y=\"StartFreq\", data=dfmergeSE, \n",
    " line_kws={'label':\"y={0:.1f}x+{1:.1f} \\n r^2={2:f}\".format(slope,intercept,r_value**2)})\n",
    "ax.figure.set_size_inches(10, 10)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.lmplot(x='Altitude', y='EndFreq', data=dfmergeSE)\n",
    "#sns.lmplot(x='Altitude', y='Diff_Norm', data=dfmergeSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = sns.barplot(y=\"Name\", x=\"Diff_Norm\", data=dfmergeSE[dfmergeSE['Diff_Norm'] < 0])\n",
    "#ax = sns.barplot(y=\"Name\", x=\"Diff_Norm\", data=dfmergeSE)\n",
    "ax = sns.barplot(y=\"Name\", x=\"Total\", data=dfmergeSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make maps of stations where the size of their bubble marker represents usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmergeSE.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmergeSE.head(5)\n",
    "#dfmergeSE.to_csv(\"stationsInfo_TripCounts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Make a map over all time of Net_Norm (StartTrips-weight*EndTrips)\n",
    "import folium\n",
    "m = folium.Map(location=[dfmergeSE['Latitude'].mean(),dfmergeSE['Longitude'].mean()], zoom_start=14)\n",
    "\n",
    "# Green means more bike trips started than ended, Blue means more ended than started\n",
    "for i in range(0,len(dfmergeSE)):\n",
    "    net_norm = dfmergeSE.iloc[i]['Diff_Norm']\n",
    "    if (net_norm>0): \n",
    "        c ='green'\n",
    "    else: \n",
    "        c = 'blue' \n",
    "\n",
    "    folium.Circle(\n",
    "      location=[dfmergeSE.iloc[i]['Latitude'], dfmergeSE.iloc[i]['Longitude']],\n",
    "      popup=dfmergeSE.iloc[i]['Name'],\n",
    "      radius=abs(net_norm)*(0.15),\n",
    "      color=c,\n",
    "      fill=True,\n",
    "      fill_color=c\n",
    "   ).add_to(m)\n",
    "#m\n",
    "#Make a map over all time of usage\n",
    "m2 = folium.Map(location=[dfmergeSE['Latitude'].mean(),dfmergeSE['Longitude'].mean()], zoom_start=14)\n",
    "\n",
    "# Red will mean more bikes ended than started, Blue means more started than ended\n",
    "for i in range(0,len(dfmergeSE)):\n",
    "    sum = dfmergeSE.iloc[i]['StartFreq'] + dfmergeSE.iloc[i]['EndFreq']\n",
    "    if (sum>0): \n",
    "        c ='crimson'\n",
    "    else: \n",
    "        c = 'blue' \n",
    "\n",
    "    folium.Circle(\n",
    "      location=[dfmergeSE.iloc[i]['Latitude'], dfmergeSE.iloc[i]['Longitude']],\n",
    "      popup=dfmergeSE.iloc[i]['Name'],\n",
    "      radius=abs(net_norm)*(0.015),\n",
    "      color=c,\n",
    "      fill=True,\n",
    "      fill_color=c\n",
    "   ).add_to(m2)\n",
    "m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2.save('totalUsage_01-2018_map.html')\n",
    "m.save('mapOfBikeODs_01_2018.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.jointplot(\"Coords_Longitude\", \"Coords_Latitude\", kind='kde', data=df)#,joint_kws=dict(gridsize=35))\n",
    "g.figure.set_size_inches(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = sns.relplot(x=\"Longitude\", y=\"Latitude\", hue=\"Net_Norm\", data=dfmergeSE);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform data into a table of Completed Routes\n",
    "(This has Trip_ID,User_ID,Start_Station,End_Station,Start_Time,End_Time, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfS = dfStart.set_index(\"Trip_ID\")\n",
    "dfE = dfEnd.set_index(\"Trip_ID\")\n",
    "dfS.head(3)\n",
    "dfE.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join on trip ID, keep coords\n",
    "#Use dfStart, dfEnd. For each trip_ID, get start & end info Lat/Long and station.\n",
    "dfTrips = dfS.join(dfE, lsuffix='_S', rsuffix='_E')\n",
    "\n",
    "#Column format:\n",
    "'''['Coords_Latitude_S', 'Coords_Longitude_S', 'Bike_Event_S', 'User_ID_S',\n",
    "       'Date_Time_S', 'Date_S', 'Time_S', 'StationName_S', 'Day_S',\n",
    "       'Coords_Latitude_E', 'Coords_Longitude_E', 'Bike_Event_E', 'User_ID_E',\n",
    "       'Date_Time_E', 'Date_E', 'Time_E', 'StationName_E', 'Day_E']'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Trip duration\n",
    "- 96.5% are under 60 minutes\n",
    "- 99.3% of trips are under 200 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at duration of trips\n",
    "def durInMinutes(row):\n",
    "    t_E= row['Date_Time_E'] \n",
    "    t_S = row['Date_Time_S']\n",
    "    dur = (t_E - t_S).seconds\n",
    "    dur = round(dur/60,2)\n",
    "    return pd.Series([dur])\n",
    "dfTrips.head(5)\n",
    "print(dfTrips)\n",
    "\n",
    "dfCompleteTrips = dfTrips.dropna(subset=['StationName_S', 'StationName_E']) #Remove incomplete trips!\n",
    "dfCompleteTrips['Duration'] = dfCompleteTrips.apply(durInMinutes, axis=1)\n",
    "# print(len(dfTrips.index)) print(len(dfCompleteTrips.index))\n",
    "# duration = (after - before) // timedelta(seconds=3600)\n",
    "print(str((len(dfCompleteTrips.index)/(len(dfTrips.index)))*100) + \"% of trips were completed. Rest were missing start or end event.\")\n",
    "dfCompleteTrips.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "sns.distplot(dfCompleteTrips[dfCompleteTrips[\"Duration\"] <= 60].Duration)\n",
    "\n",
    "\n",
    "#sns.distplot(dfCompleteTrips[dfCompleteTrips.Duration)\n",
    "#print(len(dfCompleteTrips[dfCompleteTrips[\"Duration\"] <= 20].index)/len(dfCompleteTrips.index))\n",
    "#print(len(dfCompleteTrips[dfCompleteTrips[\"Duration\"] <= 200].index)/len(dfCompleteTrips.index))\n",
    "#print(dfCompleteTrips.Duration.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCompleteTrips.to_csv('CompletedMBikeTrips.csv',index='False')\n",
    "eppley = dfCompleteTrips[(dfCompleteTrips['StationName_S'] == 'Eppley') & (dfCompleteTrips['StationName_E'] == 'Eppley')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eppley.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count number of times each route is taken!\n",
    "count_series = dfCompleteTrips.groupby(['StationName_S', 'StationName_E']).size()\n",
    "dfRoutes = count_series.to_frame(name = 'Frequency').reset_index()\n",
    "dfRoutes.to_csv(\"routeFrequency.csv\",index=False)\n",
    "dfRoutes.head(2)\n",
    "dfRoutes.sort_values(by=['Frequency'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCompleteTrips.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weekday overall hourly bike usage (Start trips)\n",
    "This data shows that bikes are rarely used for morning commutes. People are much more likely to use mBikes to commute after 12pm. This usage spikes at 9pm, and then we see that at 12am people are almost just as likely to begin a trip as at 12pm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weekday plots\n",
    "dfNew =dfCompleteTrips[(dfCompleteTrips['Day_S'] >= 0) & (dfCompleteTrips['Day_S'] <= 4)]\n",
    "dfNew.groupby([dfNew[\"Date_Time_S\"].dt.hour]).count().plot(kind=\"bar\",legend=False)\n",
    "#dfNew.groupby([dfNew[\"Date_Time_S\"].dt.year, dfNew[\"Date_Time_S\"].dt.month]).count().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWeekend =dfCompleteTrips[(dfCompleteTrips['Day_S'] >4) & (dfCompleteTrips['Day_S'] <= 6)]\n",
    "dfWeekend.groupby([dfWeekend[\"Date_Time_S\"].dt.hour]).count().plot(kind=\"bar\",legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table showing average _weekday_ bike differential by station\n",
    "Station: Morning bike diff | Midday Bike diff | Evening Bike diff | Late Bike diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into Start/Ends\n",
    "# keep only if after 01/2018 ******\n",
    "\n",
    "dfWeekdays = df[(df[\"Day\"] <=4) & (df[\"Day\"]>=0)]\n",
    "print(dfWeekdays['Date_Time'].max())\n",
    "print(dfWeekdays['Date_Time'].min())\n",
    "\n",
    "#Split into 4 dataframes, corresponding to morning, mid-day, evening and Late:\n",
    "dfWeekdaysM = dfWeekdays[(dfWeekdays['Date_Time'].dt.hour >= 4) & (dfWeekdays['Date_Time'].dt.hour < 10)]\n",
    "dfWeekdaysMid = dfWeekdays[(dfWeekdays['Date_Time'].dt.hour >= 10) & (dfWeekdays['Date_Time'].dt.hour < 16)]\n",
    "dfWeekdaysEve = dfWeekdays[(dfWeekdays['Date_Time'].dt.hour >= 16) & (dfWeekdays['Date_Time'].dt.hour < 22)]\n",
    "dfWeekdaysLate = dfWeekdays[(dfWeekdays['Date_Time'].dt.hour >= 22) | (dfWeekdays['Date_Time'].dt.hour < 4) ]\n",
    "\n",
    "#Section into time slots of 4-10am,10am-4pm, 4-10pm, 10pm-4am\n",
    "#dfStartWd1 = dfStartWd[(dfStartWd['Date_Time'].hour <= 10) & (dfStartWd['Date_Time'].hour >= 4)]\n",
    "#print(len(dfStartWd1.index))\n",
    "def getUsage(dfByTime, periodString):\n",
    "    a = dfByTime[dfByTime[\"Bike_Event\"] == 'StartTrip'].StationName.value_counts()\n",
    "    b = dfByTime[dfByTime[\"Bike_Event\"] != 'StartTrip'].StationName.value_counts()*weight\n",
    "    dfComb = pd.DataFrame(b-a)\n",
    "    periodString = periodString + ' Usage'\n",
    "    dfComb.columns = [periodString] # Negative means more end trips\n",
    "    dfComb[periodString] = dfComb[periodString].fillna(0).astype('int64')\n",
    "    return dfComb\n",
    "\n",
    "dfCombM = getUsage(dfWeekdaysM, 'Morning')\n",
    "dfCombMid = getUsage(dfWeekdaysMid, 'Mid-day')\n",
    "dfCombEve = getUsage(dfWeekdaysEve, 'Evening')\n",
    "dfCombLate = getUsage(dfWeekdaysLate, 'Late')\n",
    "\n",
    "dfCombM.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are surveying 54 weeks(Monday-monday) --> 54*5 + 1 = 271 weekdays\n",
    "dfBigCat = pd.concat([dfCombM, dfCombMid, dfCombEve, dfCombLate], axis=1, join='outer')\n",
    "dfCatPerWeekday = dfBigCat.div(271).round(1)\n",
    "dfCatPerWeekday.head(24) #dfBigCat.head(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Bike gain/loss by station per day, as well as in each section of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCatPerWeekday['Bike_Difference'] = dfCatPerWeekday.sum(axis = 1, skipna = True).round(2)\n",
    "dfCatPerWeekday.head(24)\n",
    "\n",
    "dfCatWdOrg = dfCatPerWeekday.iloc[(-np.abs(dfCatPerWeekday['Bike_Difference'].values)).argsort()]\n",
    "dfCatWdOrg.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCatWdOrg.head(24)\n",
    "dfCatWdOrg.Bike_Difference.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about during the busiest times?\n",
    "We'll look at week of Sept 3- Sept 9th 2018. 2282 Trips (started) in that week alone!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfBusy = df[(df['Date_Time'] < datetime(2018,9,10)) & (df['Date_Time'] >= datetime(2018,10,))] #After 01/2018 had 23 stations!!!!\n",
    "print(len(dfBusy[dfBusy['Bike_Event'] == 'StartTrip'].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfBusy = df[(df['Date_Time'] < datetime(2018,9,10)) & (df['Date_Time'] >= datetime(2018,9,3))] #After 01/2018 had 23 stations!!!!\n",
    "\n",
    "dfBusy = df[(df['Date_Time'] < datetime(2018,9,3)) & (df['Date_Time'] >= datetime(2018,8,27))] #After 01/2018 had 23 stations!!!!\n",
    "#dfBusy = df[(df['Date_Time'] < datetime(2018,10,8)) & (df['Date_Time'] >= datetime(2018,10,1))] #After 01/2018 had 23 stations!!!!\n",
    "\n",
    "dfWeekdaysB = dfBusy[(dfBusy[\"Day\"] <=4) & (dfBusy[\"Day\"]>=0)]\n",
    "print(dfWeekdaysB['Date_Time'].max())\n",
    "print(dfWeekdaysB['Date_Time'].min())\n",
    "\n",
    "#Split into 4 dataframes, corresponding to morning, mid-day, evening and Late:\n",
    "#This should obviously be a function that does this:\n",
    "# **************\n",
    "#Split into four periods (4-10am,10-16pm,16-22pm and 4-22pm)\n",
    "dfWeekdaysMB = dfWeekdaysB[(dfWeekdaysB['Date_Time'].dt.hour >= 4) & (dfWeekdaysB['Date_Time'].dt.hour < 10)]\n",
    "dfWeekdaysMidB = dfWeekdaysB[(dfWeekdaysB['Date_Time'].dt.hour >= 10) & (dfWeekdaysB['Date_Time'].dt.hour < 16)]\n",
    "dfWeekdaysEveB = dfWeekdaysB[(dfWeekdaysB['Date_Time'].dt.hour >= 16) & (dfWeekdaysB['Date_Time'].dt.hour < 22)]\n",
    "dfWeekdaysLateB = dfWeekdaysB[(dfWeekdaysB['Date_Time'].dt.hour >= 22) | (dfWeekdaysB['Date_Time'].dt.hour < 4) ]\n",
    "\n",
    "dfCombMB = getUsage(dfWeekdaysMB, 'Morning')\n",
    "dfCombMidB = getUsage(dfWeekdaysMidB, 'Mid-day')\n",
    "dfCombEveB = getUsage(dfWeekdaysEveB, 'Evening')\n",
    "dfCombLateB = getUsage(dfWeekdaysLateB, 'Late')\n",
    "\n",
    "dfBigCatB = pd.concat([dfCombMB, dfCombMidB, dfCombEveB, dfCombLateB], axis=1, join='outer')\n",
    "\n",
    "dfCatPerWeekdayB = dfBigCatB.div(5).round(1)\n",
    "dfCatPerWeekdayB['Bike_Difference'] = dfCatPerWeekdayB.sum(axis = 1, skipna = True).round(2)\n",
    "dfCatWdBOrg = dfCatPerWeekdayB.iloc[(-np.abs(dfCatPerWeekdayB['Bike_Difference'].values)).argsort()]\n",
    "# **************\n",
    "#and returns a data frame, dfCatWDBOrg, with each stations bike gain/loss per period of the day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCatPerWeekdayB.head(10)\n",
    "dfCatWdBOrg.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from above we have a good idea of the load balancing issues that need to be addressed, and how that varies between the average weekday and one of the busiest weekdays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
